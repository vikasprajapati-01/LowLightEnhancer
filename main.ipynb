{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fe188",
   "metadata": {},
   "source": [
    "For clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87497df4",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f86ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90438b",
   "metadata": {},
   "source": [
    "Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRAIN_IMAGES = 400\n",
    "\n",
    "TRAIN_VAL_IMAGE_DIR = \"data/lol_dataset/our485/low\"\n",
    "TEST_IMAGE_DIR = \"data/lol_dataset/eval15/low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1db5c",
   "metadata": {},
   "source": [
    "Dataset accessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_image_files = glob(os.path.join(TRAIN_VAL_IMAGE_DIR, \"*.png\"))\n",
    "test_image_files = glob(os.path.join(TEST_IMAGE_DIR, \"*.png\"))\n",
    "\n",
    "random.shuffle(train_val_image_files)\n",
    "\n",
    "train_image_files = train_val_image_files[:MAX_TRAIN_IMAGES]\n",
    "val_image_files = train_val_image_files[MAX_TRAIN_IMAGES:]\n",
    "\n",
    "print(\"Number of Training Images:\", len(train_image_files))\n",
    "print(\"Number of Validation Images:\", len(val_image_files))\n",
    "print(\"Number of Test Images from LOL Dataset:\", len(test_image_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d4b98",
   "metadata": {},
   "source": [
    "Data pairing\n",
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_dataset(images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(train_image_files)\n",
    "val_dataset = get_dataset(val_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ed9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[94m\")\n",
    "print(\"Train Data Elements:\", train_dataset.element_spec)\n",
    "print(\"Validation Data Elements:\", val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2801aa3",
   "metadata": {},
   "source": [
    "Check few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(train_dataset)).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(4, 4), axes_pad=0.1)\n",
    "\n",
    "random_images = images[np.random.choice(np.arange(images.shape[0]), 16)]\n",
    "\n",
    "for ax, image in zip(grid, random_images):\n",
    "    image = image * 255.0\n",
    "    ax.imshow(image.astype(np.uint8))\n",
    "\n",
    "plt.title(\"Sample Training Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dce_net(image_size=None) -> keras.Model:\n",
    "    input_image = keras.Input(shape=[image_size, image_size, 3])\n",
    "\n",
    "    conv1 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(input_image)\n",
    "\n",
    "    conv2 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv1)\n",
    "\n",
    "    conv3 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv2)\n",
    "\n",
    "    conv4 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv3)\n",
    "\n",
    "    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n",
    "\n",
    "    conv5 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con1)\n",
    "\n",
    "    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n",
    "\n",
    "    conv6 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con2)\n",
    "\n",
    "    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n",
    "\n",
    "    x_r = layers.Conv2D(\n",
    "        24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\"\n",
    "    )(int_con3)\n",
    "\n",
    "    return keras.Model(inputs=input_image, outputs=x_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb009f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_constancy_loss(x):\n",
    "    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
    "    mean_red = mean_rgb[:, :, :, 0]\n",
    "    mean_green = mean_rgb[:, :, :, 1]\n",
    "    mean_blue = mean_rgb[:, :, :, 2]\n",
    "\n",
    "    diff_red_green = tf.square(mean_red - mean_green)\n",
    "    diff_red_blue = tf.square(mean_red - mean_blue)\n",
    "    diff_green_blue = tf.square(mean_blue - mean_green)\n",
    "\n",
    "    return tf.sqrt(\n",
    "        tf.square(diff_red_green) +\n",
    "        tf.square(diff_red_blue) +\n",
    "        tf.square(diff_green_blue)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a37d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_loss(x, mean_val=0.6):\n",
    "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n",
    "    return tf.reduce_mean(tf.square(mean - mean_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f57886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination_smoothness_loss(x):\n",
    "    \"\"\"Inspired from https://github.com/tuvovan/Zero_DCE_TF/blob/master/src/loss.py#L28\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    height_x = tf.shape(x)[1]\n",
    "    width_x = tf.shape(x)[2]\n",
    "    count_height = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
    "    count_width = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
    "    height_total_variance = tf.reduce_sum(\n",
    "        tf.square((x[:, 1:, :, :] - x[:, : height_x - 1, :, :]))\n",
    "    )\n",
    "    width_total_variance = tf.reduce_sum(\n",
    "        tf.square((x[:, :, 1:, :] - x[:, :, : width_x - 1, :]))\n",
    "    )\n",
    "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
    "    count_height = tf.cast(count_height, dtype=tf.float32)\n",
    "    count_width = tf.cast(count_width, dtype=tf.float32)\n",
    "    return 2 * (\n",
    "        height_total_variance / count_height + width_total_variance / count_width\n",
    "    ) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d320463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialConsistencyLoss(losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SpatialConsistencyLoss, self).__init__(reduction=\"none\")\n",
    "\n",
    "        self.left_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.right_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.up_kernel = tf.constant(\n",
    "            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.down_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n",
    "        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n",
    "        original_pool = tf.nn.avg_pool2d(\n",
    "            original_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "        enhanced_pool = tf.nn.avg_pool2d(\n",
    "            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "\n",
    "        d_original_left = tf.nn.conv2d(\n",
    "            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_original_right = tf.nn.conv2d(\n",
    "            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_original_up = tf.nn.conv2d(\n",
    "            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_original_down = tf.nn.conv2d(\n",
    "            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "\n",
    "        d_enhanced_left = tf.nn.conv2d(\n",
    "            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_enhanced_right = tf.nn.conv2d(\n",
    "            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_enhanced_up = tf.nn.conv2d(\n",
    "            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_enhanced_down = tf.nn.conv2d(\n",
    "            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "\n",
    "        d_left = tf.square(d_original_left - d_enhanced_left)\n",
    "        d_right = tf.square(d_original_right - d_enhanced_right)\n",
    "        d_up = tf.square(d_original_up - d_enhanced_up)\n",
    "        d_down = tf.square(d_original_down - d_enhanced_down)\n",
    "        return d_left + d_right + d_up + d_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad705a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDCE(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ZeroDCE, self).__init__(**kwargs)\n",
    "        self.dce_model = build_dce_net(IMAGE_SIZE)\n",
    "\n",
    "    def compile(self, learning_rate, **kwargs):\n",
    "        super(ZeroDCE, self).compile(**kwargs)\n",
    "        self.optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
    "    \n",
    "    def summary(self, *args, **kwargs):\n",
    "        self.dce_model.summary(*args, **kwargs)\n",
    "\n",
    "    def get_enhanced_image(self, data, output):\n",
    "        x = data\n",
    "        for i in range(0, 3 * 8, 3):\n",
    "            r = output[:, :, :, i: i + 3]\n",
    "            x = x + r * (tf.square(x) - x)\n",
    "        return x\n",
    "\n",
    "    def call(self, data):\n",
    "        dce_net_output = self.dce_model(data)\n",
    "        return self.get_enhanced_image(data, dce_net_output)\n",
    "\n",
    "    def compute_losses(self, data, output):\n",
    "        enhanced_image = self.get_enhanced_image(data, output)\n",
    "\n",
    "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
    "\n",
    "        loss_spatial_constancy = tf.reduce_mean(\n",
    "            self.spatial_constancy_loss(enhanced_image, data)\n",
    "        )\n",
    "\n",
    "        loss_color_constancy = 5 * tf.reduce_mean(\n",
    "            color_constancy_loss(enhanced_image)\n",
    "        )\n",
    "\n",
    "        loss_exposure = 10 * tf.reduce_mean(\n",
    "            exposure_loss(enhanced_image)\n",
    "        )\n",
    "\n",
    "        total_loss = (\n",
    "            loss_illumination\n",
    "            + loss_spatial_constancy\n",
    "            + loss_color_constancy\n",
    "            + loss_exposure\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"illumination_smoothness_loss\": loss_illumination,\n",
    "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
    "            \"color_constancy_loss\": loss_color_constancy,\n",
    "            \"exposure_loss\": loss_exposure,\n",
    "        }\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.dce_model(data)\n",
    "            losses = self.compute_losses(data, output)\n",
    "\n",
    "        gradients = tape.gradient(\n",
    "            losses[\"total_loss\"],\n",
    "            self.dce_model.trainable_weights\n",
    "        )\n",
    "\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.dce_model.trainable_weights)\n",
    "        )\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def test_step(self, data):\n",
    "        output = self.dce_model(data)\n",
    "        return self.compute_losses(data, output)\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        self.dce_model.save_weights(\n",
    "            filepath, overwrite=overwrite,\n",
    "            save_format=save_format, options=options\n",
    "        )\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
    "        self.dce_model.load_weights(\n",
    "            filepath=filepath,\n",
    "            by_name=by_name,\n",
    "            skip_mismatch=skip_mismatch,\n",
    "            options=options,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_dce_model = ZeroDCE()\n",
    "zero_dce_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    original_image = original_image.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    image = keras.preprocessing.image.img_to_array(original_image)\n",
    "    image = image[:, :, :3] if image.shape[-1] > 3 else image\n",
    "\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    output_image = zero_dce_model(image, training=False)\n",
    "    output_image = tf.cast(output_image[0] * 255, dtype=tf.uint8)\n",
    "\n",
    "    output_image = Image.fromarray(output_image.numpy())\n",
    "    return output_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d81aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPredictionCallback(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, image_files, log_interval, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.image_files = image_files\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.log_interval == 0:\n",
    "            for image_file in self.image_files:\n",
    "                original_image = Image.open(image_file).convert(\"RGB\")\n",
    "                enhanced_image = infer(original_image)\n",
    "                plot_results(\n",
    "                    [original_image, enhanced_image],\n",
    "                    [\"Original\", \"Enhanced_Image\"],\n",
    "                    (15, 7),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_dce_model.compile(learning_rate=LEARNING_RATE)\n",
    "history = zero_dce_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        LogPredictionCallback(\n",
    "            image_files=random.sample(val_image_files, 4),\n",
    "            log_interval=LOG_INTERVALS\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Performance Evaluation\n",
    "\n",
    "Now that the model is trained, let's **quantitatively measure** how well it performs.\n",
    "\n",
    "We compare the **enhanced images** (model output) with the **ground truth** (well-lit images from `eval15/high/`).\n",
    "\n",
    "### Metrics Used:\n",
    "| Metric | What it Measures | Good Value |\n",
    "|--------|-----------------|------------|\n",
    "| **PSNR** (Peak Signal-to-Noise Ratio) | Pixel-level accuracy in dB | > 20 dB |\n",
    "| **SSIM** (Structural Similarity Index) | Brightness, contrast & structure similarity | > 0.80 |\n",
    "| **MAE** (Mean Absolute Error) | Average pixel difference | Close to 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2f654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5755de",
   "metadata": {},
   "source": [
    "Preprocessisng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916babb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d940a1",
   "metadata": {},
   "source": [
    "Dataset loader class ya function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dee2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ddfc85",
   "metadata": {},
   "source": [
    "Model Architecture (Convolutional Autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6248fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838b9e06",
   "metadata": {},
   "source": [
    "Output Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00468274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1889d9ca",
   "metadata": {},
   "source": [
    "Loss function (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e2866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ae7880",
   "metadata": {},
   "source": [
    "Trainng cofiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Plot Training Loss Curves\n",
    "# ============================================================================\n",
    "# WHY: Loss curves show whether the model actually LEARNED during training.\n",
    "#   - If curves go DOWN and flatten â†’ the model converged (good!)\n",
    "#   - If curves oscillate wildly â†’ the model is unstable\n",
    "#   - If curves go UP â†’ the model diverged (bad!)\n",
    "#\n",
    "# We plot all 5 losses:\n",
    "#   1. Total Loss         â€” the combined loss the optimizer minimizes\n",
    "#   2. Illumination Loss  â€” keeps the curve maps (alpha) smooth, no patchy artifacts\n",
    "#   3. Spatial Loss       â€” preserves edges and structure from the original\n",
    "#   4. Color Loss         â€” prevents unnatural color tints (e.g., too blue/green)\n",
    "#   5. Exposure Loss      â€” pushes average brightness toward a well-lit target (0.6)\n",
    "# ============================================================================\n",
    "\n",
    "loss_names = [\n",
    "    \"total_loss\",\n",
    "    \"illumination_smoothness_loss\",\n",
    "    \"spatial_constancy_loss\",\n",
    "    \"color_constancy_loss\",\n",
    "    \"exposure_loss\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Training & Validation Loss Curves\", fontsize=16, fontweight=\"bold\")\n",
    "axes = axes.flatten()\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\"]\n",
    "\n",
    "for idx, key in enumerate(loss_names):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Plot training loss for this metric\n",
    "    if key in history.history:\n",
    "        ax.plot(history.history[key], color=colors[idx], linewidth=2, label=\"Train\")\n",
    "\n",
    "    # Plot validation loss (Keras prefixes with 'val_')\n",
    "    val_key = f\"val_{key}\"\n",
    "    if val_key in history.history:\n",
    "        ax.plot(history.history[val_key], color=colors[idx],\n",
    "                linewidth=2, linestyle=\"--\", alpha=0.7, label=\"Validation\")\n",
    "\n",
    "    ax.set_title(key.replace(\"_\", \" \").title(), fontsize=12)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[5].axis(\"off\")  # Hide the unused 6th subplot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the final loss values from the last epoch\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL LOSS VALUES (Last Epoch)\")\n",
    "print(\"=\" * 60)\n",
    "for key in loss_names:\n",
    "    if key in history.history:\n",
    "        train_val = history.history[key][-1]\n",
    "        val_key = f\"val_{key}\"\n",
    "        if val_key in history.history:\n",
    "            val_val = history.history[val_key][-1]\n",
    "            print(f\"  {key:40s} | Train: {train_val:.6f} | Val: {val_val:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {key:40s} | Train: {train_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Define Performance Metric Functions\n",
    "# ============================================================================\n",
    "# These functions compute how similar the ENHANCED image is to the GROUND TRUTH.\n",
    "# We use three standard metrics from image processing research.\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def compute_psnr(enhanced, ground_truth):\n",
    "    \"\"\"\n",
    "    PSNR (Peak Signal-to-Noise Ratio)\n",
    "    ----------------------------------\n",
    "    HOW IT WORKS:\n",
    "    1. Find error at each pixel: error = enhanced - ground_truth\n",
    "    2. Compute MSE (Mean Squared Error) = average of error^2\n",
    "       â†’ Squaring penalizes large errors MORE than small ones\n",
    "    3. Convert to decibel scale: PSNR = 10 * log10(MAX^2 / MSE)\n",
    "       â†’ MAX = 1.0 since our images are normalized to [0, 1]\n",
    "\n",
    "    INTERPRETATION:\n",
    "    - > 30 dB: Excellent (nearly identical to ground truth)\n",
    "    - 25-30 dB: Good\n",
    "    - 20-25 dB: Acceptable\n",
    "    - < 20 dB: Poor\n",
    "\n",
    "    Higher PSNR = Better quality.\n",
    "    \"\"\"\n",
    "    return tf.image.psnr(enhanced, ground_truth, max_val=1.0).numpy()\n",
    "\n",
    "\n",
    "def compute_ssim(enhanced, ground_truth):\n",
    "    \"\"\"\n",
    "    SSIM (Structural Similarity Index)\n",
    "    ------------------------------------\n",
    "    HOW IT WORKS:\n",
    "    Instead of comparing individual pixels, SSIM compares small WINDOWS\n",
    "    (patches) of two images on three aspects:\n",
    "\n",
    "    1. LUMINANCE: Are both windows equally bright?\n",
    "       l = (2 * mean_x * mean_y + C1) / (mean_x^2 + mean_y^2 + C1)\n",
    "\n",
    "    2. CONTRAST: Do both windows have similar variation?\n",
    "       c = (2 * std_x * std_y + C2) / (std_x^2 + std_y^2 + C2)\n",
    "\n",
    "    3. STRUCTURE: Do both windows have similar patterns/textures?\n",
    "       s = (covariance_xy + C3) / (std_x * std_y + C3)\n",
    "\n",
    "    Final SSIM = average of (l * c * s) across all windows.\n",
    "\n",
    "    INTERPRETATION:\n",
    "    - Range: 0 to 1 (1.0 = perfectly identical)\n",
    "    - > 0.90: Excellent | 0.80-0.90: Good | < 0.60: Poor\n",
    "    \"\"\"\n",
    "    return tf.image.ssim(enhanced, ground_truth, max_val=1.0).numpy()\n",
    "\n",
    "\n",
    "def compute_mae(enhanced, ground_truth):\n",
    "    \"\"\"\n",
    "    MAE (Mean Absolute Error)\n",
    "    --------------------------\n",
    "    HOW IT WORKS:\n",
    "    Simply the average of |enhanced_pixel - ground_truth_pixel|\n",
    "    for every pixel in the image.\n",
    "\n",
    "    MAE = (1/N) * sum(|enhanced - ground_truth|)\n",
    "\n",
    "    Unlike MSE, MAE doesn't over-penalize large errors.\n",
    "    Lower = Better (0.0 = identical images)\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.abs(enhanced - ground_truth)).numpy()\n",
    "\n",
    "\n",
    "print(\"âœ… Metric functions defined: compute_psnr, compute_ssim, compute_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Evaluate Model on Test Set (eval15)\n",
    "# ============================================================================\n",
    "# This is the CORE evaluation cell. For each of the 15 test images:\n",
    "#\n",
    "# 1. Load the LOW-LIGHT image (input) from eval15/low/\n",
    "# 2. Load the GROUND TRUTH image (target) from eval15/high/\n",
    "# 3. Pass the low-light image through our trained model â†’ ENHANCED image\n",
    "# 4. Compute PSNR, SSIM, MAE between:\n",
    "#    a) ORIGINAL (low) vs GROUND TRUTH  â†’ baseline (how bad the input is)\n",
    "#    b) ENHANCED vs GROUND TRUTH        â†’ model performance\n",
    "#\n",
    "# The IMPROVEMENT = (b) - (a) tells us how much the model helped.\n",
    "# ============================================================================\n",
    "\n",
    "# Paths to the test set â€” these images were NOT used during training\n",
    "TEST_LOW_DIR = \"data/lol_dataset/eval15/low\"\n",
    "TEST_HIGH_DIR = \"data/lol_dataset/eval15/high\"\n",
    "\n",
    "# Get sorted file lists so low[i] matches high[i]\n",
    "test_low_files = sorted(glob(os.path.join(TEST_LOW_DIR, \"*.png\")))\n",
    "test_high_files = sorted(glob(os.path.join(TEST_HIGH_DIR, \"*.png\")))\n",
    "\n",
    "print(f\"Evaluating on {len(test_low_files)} test images...\\n\")\n",
    "\n",
    "# Store results for each image\n",
    "eval_results = {\n",
    "    \"filenames\": [],\n",
    "    \"psnr_original\": [], \"ssim_original\": [], \"mae_original\": [],\n",
    "    \"psnr_enhanced\": [], \"ssim_enhanced\": [], \"mae_enhanced\": [],\n",
    "}\n",
    "\n",
    "for low_path, high_path in zip(test_low_files, test_high_files):\n",
    "    filename = os.path.basename(low_path)\n",
    "\n",
    "    # --- Step 1: Load low-light image and normalize to [0, 1] ---\n",
    "    low_img = tf.io.read_file(low_path)\n",
    "    low_img = tf.image.decode_png(low_img, channels=3)\n",
    "    low_img = tf.image.resize(low_img, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    low_img = tf.cast(low_img, tf.float32) / 255.0\n",
    "\n",
    "    # --- Step 2: Load ground truth (well-lit) image ---\n",
    "    high_img = tf.io.read_file(high_path)\n",
    "    high_img = tf.image.decode_png(high_img, channels=3)\n",
    "    high_img = tf.image.resize(high_img, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    high_img = tf.cast(high_img, tf.float32) / 255.0\n",
    "\n",
    "    # --- Step 3: Enhance the low-light image using our trained model ---\n",
    "    # The model expects a batch, so we add a dimension: (H,W,3) â†’ (1,H,W,3)\n",
    "    low_batch = tf.expand_dims(low_img, axis=0)\n",
    "    enhanced_img = zero_dce_model(low_batch, training=False)\n",
    "    # Clip to valid range [0, 1] to avoid display artifacts\n",
    "    enhanced_img = tf.clip_by_value(enhanced_img[0], 0.0, 1.0)\n",
    "\n",
    "    # --- Step 4: Compute metrics ---\n",
    "    # Add batch dimension for TF metric functions\n",
    "    enhanced_batch = tf.expand_dims(enhanced_img, 0)\n",
    "    high_batch = tf.expand_dims(high_img, 0)\n",
    "\n",
    "    # Original (low-light) vs Ground Truth â€” the BASELINE\n",
    "    psnr_o = compute_psnr(low_batch, high_batch)[0]\n",
    "    ssim_o = compute_ssim(low_batch, high_batch)[0]\n",
    "    mae_o = compute_mae(low_img, high_img)\n",
    "\n",
    "    # Enhanced vs Ground Truth â€” the MODEL'S PERFORMANCE\n",
    "    psnr_e = compute_psnr(enhanced_batch, high_batch)[0]\n",
    "    ssim_e = compute_ssim(enhanced_batch, high_batch)[0]\n",
    "    mae_e = compute_mae(enhanced_img, high_img)\n",
    "\n",
    "    # Store results\n",
    "    eval_results[\"filenames\"].append(filename)\n",
    "    eval_results[\"psnr_original\"].append(psnr_o)\n",
    "    eval_results[\"ssim_original\"].append(ssim_o)\n",
    "    eval_results[\"mae_original\"].append(mae_o)\n",
    "    eval_results[\"psnr_enhanced\"].append(psnr_e)\n",
    "    eval_results[\"ssim_enhanced\"].append(ssim_e)\n",
    "    eval_results[\"mae_enhanced\"].append(mae_e)\n",
    "\n",
    "    # Print per-image results with arrows showing improvement\n",
    "    print(f\"  {filename:20s} | \"\n",
    "          f\"PSNR: {psnr_o:.2f} â†’ {psnr_e:.2f} dB | \"\n",
    "          f\"SSIM: {ssim_o:.4f} â†’ {ssim_e:.4f} | \"\n",
    "          f\"MAE: {mae_o:.4f} â†’ {mae_e:.4f}\")\n",
    "\n",
    "# --- Print Summary ---\n",
    "avg_psnr_o = np.mean(eval_results[\"psnr_original\"])\n",
    "avg_psnr_e = np.mean(eval_results[\"psnr_enhanced\"])\n",
    "avg_ssim_o = np.mean(eval_results[\"ssim_original\"])\n",
    "avg_ssim_e = np.mean(eval_results[\"ssim_enhanced\"])\n",
    "avg_mae_o = np.mean(eval_results[\"mae_original\"])\n",
    "avg_mae_e = np.mean(eval_results[\"mae_enhanced\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  PERFORMANCE SUMMARY â€” Enhanced vs Ground Truth (Test Set)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Metric':<25s} | {'Original (Low)':<15s} | {'Enhanced':<15s} | {'Improvement':<15s}\")\n",
    "print(f\"  {'-'*25}-+-{'-'*15}-+-{'-'*15}-+-{'-'*15}\")\n",
    "print(f\"  {'PSNR (dB) â†‘ better':<25s} | {avg_psnr_o:>13.2f}  | {avg_psnr_e:>13.2f}  | +{avg_psnr_e - avg_psnr_o:.2f} dB\")\n",
    "print(f\"  {'SSIM (0-1) â†‘ better':<25s} | {avg_ssim_o:>13.4f}  | {avg_ssim_e:>13.4f}  | +{avg_ssim_e - avg_ssim_o:.4f}\")\n",
    "print(f\"  {'MAE (0-1) â†“ better':<25s} | {avg_mae_o:>13.4f}  | {avg_mae_e:>13.4f}  | {avg_mae_e - avg_mae_o:+.4f}\")\n",
    "print(\"\\n  Interpretation:\")\n",
    "print(\"  â€¢ PSNR > 20 dB: Acceptable  |  > 25 dB: Good  |  > 30 dB: Excellent\")\n",
    "print(\"  â€¢ SSIM > 0.60: Moderate  |  > 0.80: Good  |  > 0.90: Excellent\")\n",
    "print(\"  â€¢ MAE closer to 0 = better pixel-level accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Bar Chart â€” Per-Image Metric Comparison\n",
    "# ============================================================================\n",
    "# This creates side-by-side bar charts comparing ORIGINAL vs ENHANCED\n",
    "# for each test image. The RED bars (original) should be worse than\n",
    "# the GREEN bars (enhanced) for PSNR and SSIM.\n",
    "# For MAE, GREEN bars should be LOWER (shorter) than RED.\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"Per-Image Metrics: Original vs Enhanced\",\n",
    "             fontsize=16, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "x = np.arange(len(eval_results[\"filenames\"]))\n",
    "width = 0.35  # Width of each bar\n",
    "\n",
    "# --- PSNR Bar Chart (Higher = Better) ---\n",
    "axes[0].bar(x - width/2, eval_results[\"psnr_original\"], width,\n",
    "            label=\"Original (Low)\", color=\"#e74c3c\", alpha=0.8)\n",
    "axes[0].bar(x + width/2, eval_results[\"psnr_enhanced\"], width,\n",
    "            label=\"Enhanced\", color=\"#2ecc71\", alpha=0.8)\n",
    "axes[0].set_title(\"PSNR (dB) â€” Higher is Better â†‘\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Test Image\")\n",
    "axes[0].set_ylabel(\"PSNR (dB)\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f.split('.')[0] for f in eval_results[\"filenames\"]],\n",
    "                        rotation=45, ha=\"right\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# --- SSIM Bar Chart (Higher = Better) ---\n",
    "axes[1].bar(x - width/2, eval_results[\"ssim_original\"], width,\n",
    "            label=\"Original (Low)\", color=\"#e74c3c\", alpha=0.8)\n",
    "axes[1].bar(x + width/2, eval_results[\"ssim_enhanced\"], width,\n",
    "            label=\"Enhanced\", color=\"#2ecc71\", alpha=0.8)\n",
    "axes[1].set_title(\"SSIM â€” Higher is Better â†‘\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Test Image\")\n",
    "axes[1].set_ylabel(\"SSIM\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f.split('.')[0] for f in eval_results[\"filenames\"]],\n",
    "                        rotation=45, ha=\"right\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "\n",
    "# --- MAE Bar Chart (Lower = Better) ---\n",
    "axes[2].bar(x - width/2, eval_results[\"mae_original\"], width,\n",
    "            label=\"Original (Low)\", color=\"#e74c3c\", alpha=0.8)\n",
    "axes[2].bar(x + width/2, eval_results[\"mae_enhanced\"], width,\n",
    "            label=\"Enhanced\", color=\"#2ecc71\", alpha=0.8)\n",
    "axes[2].set_title(\"MAE â€” Lower is Better â†“\", fontsize=13, fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Test Image\")\n",
    "axes[2].set_ylabel(\"MAE\")\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels([f.split('.')[0] for f in eval_results[\"filenames\"]],\n",
    "                        rotation=45, ha=\"right\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Side-by-Side Visual Comparison with Annotated Metrics\n",
    "# ============================================================================\n",
    "# This is the most IMPRESSIVE cell for your teacher/judge.\n",
    "# It shows 5 test images as:\n",
    "#   [LOW-LIGHT input]  â†’  [ENHANCED by model]  â†’  [GROUND TRUTH target]\n",
    "# with PSNR, SSIM, MAE values printed above each enhanced image.\n",
    "#\n",
    "# WHAT TO LOOK FOR:\n",
    "# - The enhanced image should be BRIGHTER than the original\n",
    "# - The enhanced image should look SIMILAR to the ground truth\n",
    "# - Colors should be natural (no weird tints)\n",
    "# - Details/edges should be preserved (not blurry)\n",
    "# ============================================================================\n",
    "\n",
    "num_display = min(5, len(test_low_files))  # Show up to 5 images\n",
    "\n",
    "fig, axes = plt.subplots(num_display, 3, figsize=(15, 5 * num_display))\n",
    "fig.suptitle(\"Visual Comparison: Original â†’ Enhanced â†’ Ground Truth\",\n",
    "             fontsize=18, fontweight=\"bold\", y=1.01)\n",
    "\n",
    "for row in range(num_display):\n",
    "    low_path = test_low_files[row]\n",
    "    high_path = test_high_files[row]\n",
    "\n",
    "    # Load and normalize images\n",
    "    low_img = tf.cast(tf.image.resize(\n",
    "        tf.image.decode_png(tf.io.read_file(low_path), channels=3),\n",
    "        [IMAGE_SIZE, IMAGE_SIZE]), tf.float32) / 255.0\n",
    "\n",
    "    high_img = tf.cast(tf.image.resize(\n",
    "        tf.image.decode_png(tf.io.read_file(high_path), channels=3),\n",
    "        [IMAGE_SIZE, IMAGE_SIZE]), tf.float32) / 255.0\n",
    "\n",
    "    # Enhance\n",
    "    enhanced_img = zero_dce_model(tf.expand_dims(low_img, 0), training=False)\n",
    "    enhanced_img = tf.clip_by_value(enhanced_img[0], 0.0, 1.0)\n",
    "\n",
    "    # Compute metrics for this image\n",
    "    e_batch = tf.expand_dims(enhanced_img, 0)\n",
    "    h_batch = tf.expand_dims(high_img, 0)\n",
    "    psnr_val = compute_psnr(e_batch, h_batch)[0]\n",
    "    ssim_val = compute_ssim(e_batch, h_batch)[0]\n",
    "    mae_val = compute_mae(enhanced_img, high_img)\n",
    "\n",
    "    # Plot the three versions side by side\n",
    "    axes[row, 0].imshow(low_img.numpy())\n",
    "    axes[row, 0].set_title(\"Low-Light (Input)\", fontsize=12)\n",
    "    axes[row, 0].axis(\"off\")\n",
    "\n",
    "    axes[row, 1].imshow(enhanced_img.numpy())\n",
    "    axes[row, 1].set_title(\n",
    "        f\"Enhanced\\nPSNR: {psnr_val:.2f} dB | SSIM: {ssim_val:.4f} | MAE: {mae_val:.4f}\",\n",
    "        fontsize=11, color=\"green\"\n",
    "    )\n",
    "    axes[row, 1].axis(\"off\")\n",
    "\n",
    "    axes[row, 2].imshow(high_img.numpy())\n",
    "    axes[row, 2].set_title(\"Ground Truth\", fontsize=12)\n",
    "    axes[row, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Brightness Histogram Comparison\n",
    "# ============================================================================\n",
    "# A histogram shows the DISTRIBUTION of brightness values in an image.\n",
    "#\n",
    "# WHAT TO LOOK FOR:\n",
    "# - Original (red curve): Clustered on the LEFT (near 0 = dark pixels)\n",
    "#   â†’ This confirms the image is indeed dark/low-light\n",
    "#\n",
    "# - Enhanced (green curve): Should SHIFT TO THE RIGHT (brighter)\n",
    "#   â†’ The model is successfully brightening the image\n",
    "#\n",
    "# - Ground Truth (blue curve): The \"target\" brightness distribution\n",
    "#   â†’ If the green curve closely matches the blue curve, the model\n",
    "#     is doing a great job at matching the correct brightness!\n",
    "#\n",
    "# This analysis gives VISUAL PROOF that brightness is being corrected.\n",
    "# ============================================================================\n",
    "\n",
    "num_hist = min(3, len(test_low_files))  # Show histograms for 3 images\n",
    "\n",
    "fig, axes = plt.subplots(num_hist, 2, figsize=(16, 5 * num_hist))\n",
    "fig.suptitle(\"Brightness Histogram Analysis\",\n",
    "             fontsize=18, fontweight=\"bold\", y=1.01)\n",
    "\n",
    "for row in range(num_hist):\n",
    "    low_path = test_low_files[row]\n",
    "    high_path = test_high_files[row]\n",
    "\n",
    "    # Load images\n",
    "    low_img = tf.cast(tf.image.resize(\n",
    "        tf.image.decode_png(tf.io.read_file(low_path), channels=3),\n",
    "        [IMAGE_SIZE, IMAGE_SIZE]), tf.float32) / 255.0\n",
    "\n",
    "    high_img = tf.cast(tf.image.resize(\n",
    "        tf.image.decode_png(tf.io.read_file(high_path), channels=3),\n",
    "        [IMAGE_SIZE, IMAGE_SIZE]), tf.float32) / 255.0\n",
    "\n",
    "    # Enhance\n",
    "    enhanced_img = zero_dce_model(tf.expand_dims(low_img, 0), training=False)\n",
    "    enhanced_img = tf.clip_by_value(enhanced_img[0], 0.0, 1.0)\n",
    "\n",
    "    # Convert to grayscale brightness (average across RGB channels)\n",
    "    # This gives us a single brightness value per pixel\n",
    "    low_gray = tf.reduce_mean(low_img, axis=-1).numpy().flatten()\n",
    "    enhanced_gray = tf.reduce_mean(enhanced_img, axis=-1).numpy().flatten()\n",
    "    high_gray = tf.reduce_mean(high_img, axis=-1).numpy().flatten()\n",
    "\n",
    "    # LEFT: Show the three image versions side by side\n",
    "    combined = np.concatenate([\n",
    "        low_img.numpy(), enhanced_img.numpy(), high_img.numpy()\n",
    "    ], axis=1)\n",
    "    axes[row, 0].imshow(combined)\n",
    "    axes[row, 0].set_title(\"Original  |  Enhanced  |  Ground Truth\", fontsize=12)\n",
    "    axes[row, 0].axis(\"off\")\n",
    "\n",
    "    # RIGHT: Plot overlapping histograms\n",
    "    axes[row, 1].hist(low_gray, bins=100, alpha=0.5, color=\"#e74c3c\",\n",
    "                      label=\"Original (Low)\", density=True)\n",
    "    axes[row, 1].hist(enhanced_gray, bins=100, alpha=0.5, color=\"#2ecc71\",\n",
    "                      label=\"Enhanced\", density=True)\n",
    "    axes[row, 1].hist(high_gray, bins=100, alpha=0.5, color=\"#3498db\",\n",
    "                      label=\"Ground Truth\", density=True)\n",
    "    axes[row, 1].set_title(f\"Brightness Distribution â€” {os.path.basename(low_path)}\",\n",
    "                           fontsize=12)\n",
    "    axes[row, 1].set_xlabel(\"Pixel Brightness (0 = black, 1 = white)\")\n",
    "    axes[row, 1].set_ylabel(\"Density\")\n",
    "    axes[row, 1].legend()\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "    axes[row, 1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: Final Evaluation Report\n",
    "# ============================================================================\n",
    "# A clean, comprehensive summary of the model's performance.\n",
    "# Perfect for including in your project report or showing to the teacher.\n",
    "# ============================================================================\n",
    "\n",
    "avg_psnr_o = np.mean(eval_results[\"psnr_original\"])\n",
    "avg_psnr_e = np.mean(eval_results[\"psnr_enhanced\"])\n",
    "avg_ssim_o = np.mean(eval_results[\"ssim_original\"])\n",
    "avg_ssim_e = np.mean(eval_results[\"ssim_enhanced\"])\n",
    "avg_mae_o = np.mean(eval_results[\"mae_original\"])\n",
    "avg_mae_e = np.mean(eval_results[\"mae_enhanced\"])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"   FINAL EVALUATION REPORT â€” Zero-DCE Low-Light Enhancement\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"  Model Architecture : Zero-DCE (Zero-Reference Deep Curve Estimation)\")\n",
    "print(\"  Training Approach  : Unsupervised (no paired data needed for training)\")\n",
    "print(f\"  Training Images    : {MAX_TRAIN_IMAGES}\")\n",
    "print(f\"  Test Images        : {len(eval_results['filenames'])}\")\n",
    "print(f\"  Image Size         : {IMAGE_SIZE} x {IMAGE_SIZE}\")\n",
    "print(f\"  Epochs             : {EPOCHS}\")\n",
    "print(f\"  Learning Rate      : {LEARNING_RATE}\")\n",
    "print(f\"  Batch Size         : {BATCH_SIZE}\")\n",
    "print()\n",
    "print(f\"  {'Metric':<15s} | {'Before (Low)':<14s} | {'After (Enhanced)':<16s} | {'Improvement':<15s}\")\n",
    "print(f\"  {'-'*15}-+-{'-'*14}-+-{'-'*16}-+-{'-'*15}\")\n",
    "print(f\"  {'PSNR (dB)':<15s} | {avg_psnr_o:>12.2f}  | {avg_psnr_e:>14.2f}  | +{avg_psnr_e - avg_psnr_o:.2f} dB\")\n",
    "print(f\"  {'SSIM':<15s} | {avg_ssim_o:>12.4f}  | {avg_ssim_e:>14.4f}  | +{avg_ssim_e - avg_ssim_o:.4f}\")\n",
    "print(f\"  {'MAE':<15s} | {avg_mae_o:>12.4f}  | {avg_mae_e:>14.4f}  | {avg_mae_e - avg_mae_o:+.4f}\")\n",
    "print()\n",
    "print(\"  Interpretation Guide:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "print(\"  PSNR  â†’ > 20 dB: Acceptable | > 25: Good | > 30: Excellent\")\n",
    "print(\"  SSIM  â†’ > 0.60: Moderate | > 0.80: Good | > 0.90: Excellent\")\n",
    "print(\"  MAE   â†’ Closer to 0.0 is better\")\n",
    "print()\n",
    "print(\"  Key Observations:\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "print(\"  â€¢ The model enhances low-light images WITHOUT using paired training data.\")\n",
    "print(\"  â€¢ It learns enhancement curves purely from unsupervised loss functions:\")\n",
    "print(\"    - Exposure Loss: targets a well-lit brightness level\")\n",
    "print(\"    - Color Constancy: prevents unnatural color shifts\")\n",
    "print(\"    - Illumination Smoothness: ensures smooth, artifact-free enhancement\")\n",
    "print(\"    - Spatial Consistency: preserves edges and structural details\")\n",
    "print(\"  â€¢ Ground truth images are used ONLY for evaluation, not training.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
